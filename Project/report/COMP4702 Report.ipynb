{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08aa1125-a7f9-4096-96d8-b848fd86d153",
   "metadata": {},
   "source": [
    "COMP4702 Report\n",
    "================\n",
    "\n",
    "Student: Hugo Burton\n",
    "\n",
    "Date Due: Friday $24^{\\text{th}}$ May 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c10bd-44a3-4765-8de8-e6cf4b27ad83",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Introduction to the dataaset\n",
    "\n",
    "This report is an investigation into the quantitative traits of fruit flies, known scientifically as the species _Drosophila aldrichi_ and _D. buzzatii_. The data, collected in 1994 captures a range of traits of $n = 1731$ flies located on the East coast of Australia. The attached report on the dataset aims to contribute to understanding the factors driving latitudinal variation in size-related traits in the two species, considering both genetic adaptation and phenotypic plasticity in response to environmental conditions.\n",
    "\n",
    "Fruit flies are a pest to many kinds of agriculture. Adult female flies lay their eggs within fruit and vegetable crops leading to their larvae hatching inside. These larvae then proceed to each the fruit from the inside out, causing rotting and thus destroying the crop. (Department of Agriculture, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b9dcf-3786-4434-9893-d5c04206449c",
   "metadata": {},
   "source": [
    "## Preliminary Observations of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538cf7a8-800f-40b9-99b9-da2ed520df33",
   "metadata": {},
   "source": [
    "Prior to determining an objective for this report, the dataset was analysed from a purely morphological perspective.\n",
    "\n",
    "### Thorax and Wing Traits Dataset\n",
    "\n",
    "A correlation plot of the `Thorax & wing traits` dataset is shown below. Evidently the columns species, population, temperature and sex all show a small magnitude of correlation with columns Thorax length through to wing_loading. \n",
    "\n",
    "Longitude and lattitude do show some correlation (other than with each other) - this could be interesting to investigate, though may not produce the best results from a classifier with this data alone.\n",
    "\n",
    "The columns `Thorax_length` through to `wing_loading` show a highly positive correlation as seen in the bottom-right of the matrix, indicating these variables are very tightly related to each other. For example, we could say a fly with a higher than average Thorax length also very likely has a higher then average wing loading.\n",
    "\n",
    "We note some columns as having no correlation with any other columns, namely `year_start` and `year_end`. Looking at the raw data, we see these are constant for all rows. In any analysis it would be a good decision to strip these from the model as they don't provide any additional information for classification purposes. Additionally, it was observed that there exist only distinct `Latitude` and `Longitude` which are unique for each population. To make the classification fair, it would also make sense to remove these prior to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded27d65-a7a1-4e93-95fb-a595e31ad602",
   "metadata": {},
   "source": [
    "##### Thorax and Wing Traits Correlation matrix\n",
    "\n",
    "<img src=\"..\\plots\\Thorax_corr_matrix.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b030e-0dac-4d02-bb61-60bf2b9e85b3",
   "metadata": {},
   "source": [
    "The `Wing Traits and Asymmetry` dataset does show correlation between each of the wing asymmetry traits, however, it shows little to no correlation between wing asymmetry and species and population. Meaning it is potentially not a good candidate dataset for making predictions about the fly's species and population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79481be-6245-4b8d-902d-f12bf88c0dcb",
   "metadata": {},
   "source": [
    "#### Wing Traits and Asymmetry Correlation Matrix\n",
    "\n",
    "<img src=\"..\\plots\\Wing_asymmetry_corr_matrix.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6b0ac-b334-4d6a-a0cb-192e339c2f85",
   "metadata": {},
   "source": [
    "The `Wing Asymmetry` dataset (which contains different asymmetry traits from the previous dataset) does show signs of correlation between wing attributes and species & population, though this isn't as strong as the first dataset. It may, however, be a good candidate for determining temperature if that were of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303336a-113f-4b4c-9fa1-e4331dc5cb0b",
   "metadata": {},
   "source": [
    "#### Wing Asymmetry Correlation Matrix\n",
    "\n",
    "<img src=\"..\\plots\\Wing_traits_corr_matrix.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b87514-fc45-4171-b84d-b65024e752c1",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "While the report attached to these datasets (not this report) does provide explanation of what it is investigating, it does not provide detail on the reasoning behind doing so, other than simply to contribute to knowledge. This report will attempt to develop models which can classify both the **Species** and **Population** of fruit flies given the remaining variables collected. As per the preliminary analysis of the data, the columns `Latitude`, `Longitude`, `Year_start`, and `Year_end` will be removed from the dataset prior to fitting the models.\n",
    "\n",
    "Secondly, the report will determine a classifier which performs the best for the above object and discuss reasoning for any findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff1be0-1c7c-498f-8a25-80eecc6a41ab",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The objective defined above is related to classification of the **Loeschcke** dataset, meaning only classification models will be used. In this report, the following classifiers are:\n",
    "- $k$-NN\n",
    "- Decision Tree\n",
    "- Neural Network\n",
    "\n",
    "\n",
    "Where possible each model will be tested equally on the dataset. Note that in the case of a neural network, a single network can output multiple variables. However, this is not possible with $k$-NN or a decision tree, hence multiple classifiers will be instantiated and combined in order to make these predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc270e5b-989a-40b0-a917-4ca7ff0d0c60",
   "metadata": {},
   "source": [
    "## Hardware\n",
    "\n",
    "The models were trained on the following hardware\n",
    "\n",
    "- Intel&reg; Core&trade; i7 12700K @ 5.0 GHz\n",
    "- 32GB DDR5 @ 5200 MT/s\n",
    "- Samsung 990 Pro 1TB\n",
    "- Asus ROG STRIX GeForce RTX 3080-Ti OC @ 1940 MHz with 12GB GDDR6X @ 9630 MT/s\n",
    "- Windows 11 Pro\n",
    "\n",
    "CUDA compute was used to train the neural network model in a conda enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d31ba-ddf5-4293-b141-0c9cbdef3585",
   "metadata": {},
   "source": [
    "## Processing of the Dataset\n",
    "\n",
    "In addition to filtering some of the features out, the data was preprocessed before it was fed to the models. In order of implementation, the following preprocessing steps were performed on the `Thorax and wing traits` dataset.\n",
    "\n",
    "### Randomisation\n",
    "\n",
    "For a handful of the rows of data, some columns were missing values. Given this is fairly insignificant with respect to the $n=1731$ rows in the dataset, any decision would be acceptable here, however, in this case these values were replaced with the mean of the respective column. Replacing with the mean is a logical decision as in theory this should not have an affect the distribution of the data.\n",
    "\n",
    "### Test Train Split\n",
    "\n",
    "For all models, a $70$ to $30$ train to test split was performed on the data. However, each model used the data slightly differently, and is further discussed below in the **Models** section of this report.\n",
    "\n",
    "### Normalisation\n",
    "\n",
    "For both non-parametric models, the data was normalised prior to fitting the respective classifier. In the case of $k$-NN, this is an essential step to ensure the distances commputed by the model use the Euclidean distance metric. Hence we want each feature to have (approximately) the same variance such that distances between variables are uniform. and decision tree models, the data was normalised, primarily because normalising does not have any affect on the model's performance. For the decision tree, this is less important, though given this is still a non-parametric learning algorithm, it remains standard practice.\n",
    "\n",
    "### Encoding of non-numeric data\n",
    "\n",
    "For the three columns `Species`, `Population`, and `Sex`, the string values were converted into indices (e.g. $0, 1, 2, ...$) so each model could interpret them. These columns are not normalised since the values take discrete values.\n",
    "\n",
    "### One hot encoding of the outputs for the neural network.\n",
    "\n",
    "In the case of the neural network model the output variables $(y)$ were one-hot encoded. This is discussed more in detail in later sections of this report, but this was primarily done in order to enable the network to produce an output for which a loss can easily be computed. It would not make sense for discrete classes $(0, 1, 2, ...)$ to have different \"losses\" between them simply because the index of the class was further apart vs closer. By one-hot-encoding the output variables, each pair of classes has the same loss magnitude between them unlike numerical data where there should be a metric of loss which is proportional to the \"distance\" between values.\n",
    "\n",
    "OHE also enables multiple variables to be more easily predicted, as these can be outputted in a flattened vector far more easily by a model.\n",
    "\n",
    "### PCA?\n",
    "\n",
    "Although PCA is discussed heavily in machine learning, it was not performed for this classification task. Primarily, there are already few features ($14$) and so further reducing the dimensionality of the data does not make sense in this case. Furthermore, a consequence of performing PCA is the loss of interpretability. For example when visualising decision boundaries, a single principal component may be comprised of many original variables meaning one cannot say for sure which of those variables contributed to the decision boundary being at a given place on the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d3aec-709a-4f1d-ba67-851814dbc1ff",
   "metadata": {},
   "source": [
    "## $k$-NN\n",
    "\n",
    "### Model Setup\n",
    "\n",
    "As two output variables are being predicted, two separate $k$-NN classifiers were constructed, each with equal parameter setups (e.g. train set, test set, cross validation method, etc).\n",
    "\n",
    "### Selection of $k$ - Cross Validation\n",
    "\n",
    "Cross validation was performed for each of the two $k$-NN classifiers (one for each output variable). A grid search over a wide range of $k$ values was performed in order to find the optimal value of $k$. To start, a $30\\%$ test set was put aside and kept independent from cross validation. On the remaining $70\\%$, each $k$ from $1$ through $100$ was trained on a randomly selected $\\frac{4}{5}$ of the train set and validated on the remaining $\\frac{1}{5}$. For each $k$, a new slicing into $5$ buckets was performed. The validation allowed us to obtain an accuracy value for each $k$ in the tested range. Once found, a final classifier was trained on a separate test set (not used in cross validation) to obtain the final results. A plot showing the accuracies for the output variable `Species` (from cross validation) of each $k$ is below:\n",
    "\n",
    "<img src=\"..\\plots\\Thorax_knn_accuracies_Species.png\" width=\"700\">\n",
    "\n",
    "The optimal value of $k$ found was $k = 34$. We observe the accuracy plot, though on a fine scale is not a smooth function. Partially this is due to the dataset being only $n = 1731$ rows which when sliced up into discrete bins used for training reduces the size of each effective training set significantly. Therefore, a small change in $k$ can result in a big change in accuracy due to being \"lucky\" or \"unlucky\" in classification being far more likely. Nonetheless, this does show a clear maximum on the lower end of the x-axis.\n",
    "\n",
    "A plot showing the accuracies for the output variable `Population` is shown below. Note the same range of $k$ was tested.\n",
    "\n",
    "<img src=\"..\\plots\\Thorax_knn_accuracies_Population.png\" width=\"700\">\n",
    "\n",
    "We see from this graph that the optimal value of $k$ was slightly higher at $k = 39$. This indicates population classes are likely more clustered than species. The reasoning behind this being that classes which are more clustered are less likely to decrease in performance with higher $k$ values due to having a majority of that class within a given region in hyperspace. Compared with if the classes are more overlapping or have scattered clusters, a change in $k$, even by $\\Delta k = 1$ could significantly change the accuracy score, and we see this evident in the `Species` classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13926f68-c1b6-4392-85e9-f33b65c51f88",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "The decision tree model had a similar setup to $k$-NN, however without cross validation being used. Again, two decision tree classifiers were trained since there were two output variables. Due to the small number of features ($14$), the only hyperparameter of the max tree height was chosen at $h_{\\text{max}} = 10$.\n",
    "\n",
    "However, the decision tree was utilised for determining the importance of variables or predictors in the dataset. This was done independently for each of the two output variables. When training a decision tree, a ranking of the variables is naturally provided by the `sklearn` library. The importance of a feature is calculated based on the total reduction of the criterion (e.g., Gini impurity) brought by that feature. For each feature, `scikit-learn` under the hood sums up the reduction in impurity for all the nodes where the feature is used to split the data. These feature scores are then normalised providing a vector of relative feature importances as an output.\n",
    "\n",
    "There are other methods of determining variable importance, including for example Generalised Cross Validation (GCV) which essentially computes the reduction in the GCV metric as each variables is added into the classifier. The value of GCV is derived from leave-one-out cross validation (LOOCV) error and can be calculated with the following formula:\n",
    "\n",
    "$$GCV(\\theta) = \\frac{1}{N}\\sum_{i=1}{N}\\left(\\frac{y_i - \\hat{f}_\\theta^{(-i)}(x_i)}{1-\\hat{h}_i}\\right)^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $GCV(\\theta)$ is the GCV score for a given hyperparameter value. \n",
    "- $N$ is the number of data points in the dataset\n",
    "- $y_i$ is the observed or true target value (class) for the $i$-th data point.\n",
    "- $\\hat{f}_\\theta^{(-i)}$ is the model's prediction for the $i$-th data point when that data point is omitted (i.e. LOOCV)\n",
    "- $\\hat{h}_i$ is a measure of influence of the $i$-th data point in the model. For example it could be distance from the mean for each variable \n",
    "\n",
    "... however, this is fairly complex to implement and I don't believe there exists a function to do this in `sklearn` or `scikit-learn` like there is in R.\n",
    "\n",
    "Hence when training both the $k$-NN and neural network models, the decision tree was trained on the same data first, not for it's performance, but instead to obtain a ranking of predictor variables. This was then used in order to determine which decision boundary plots to show, given that plotting all of them would be infeasible and difficult to understand. Note the ranking of predictor variables is independent between output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52a48d-4e25-45bc-abc3-8fa4b108c55d",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "A neural network model has the ablity to output multiple variables simultaneously. As multiple variables were being classified in this report, the neural network model did have an advantage here, though the implementation complexity is significantly higher. \n",
    "\n",
    "In order to output multiple class variables, $y$ was flattened into $\\left[\\text{data rows}, \\text{output var}, \\text{one-hot-encoded value}\\right]$. Note that in numpy arrays / torch tensors, lengths within an axis must remain constant, so the where species has only two outputs, population has five. A workaround is to pad the one-hot-encodings to be the same length between variables. \n",
    "\n",
    "Computing the loss then becomes trivial since this is just a difference of two one-hot-encoded vectors - one the true $y$ and the other then predictions (either for training, validation or test). However, in order to compute accuracy, the original $y$'s (not one-hot-encoded) needed to be recovered. This also enables visualisation of the model in a decision boundary plot shown in a later section of this report.\n",
    "\n",
    "The model was saved meaning it could be loaded at a later stage without retraining in order to make predictions.\n",
    "\n",
    "The following hyperparameters were used\n",
    "- epochs = $8 \\cdot 10^4$\n",
    "- batch_size = $1000$\n",
    "- learning rate $2 \\cdot 10^{-4}$\n",
    "- weight decay = $0.01$.\n",
    "\n",
    "The number of epochs and learning rate was chosen as this achieved a good looking accuracy curve function which converged, and remained converged during the first quarter of epochs for the duration of training. The batch size is chosen arbitrarily here and more comes down to resource capacity than anything else. The Adam optimiser was used, along with Cross Entropy for the loss function.\n",
    "\n",
    "Note the weight decay value is the $\\lambda$ which controls how much $L2$ penalty the optimiser has. A very low value was chosen here because we are already very limited for the number of features $14$ inputted into the model. Even a simple neural network model like the one used in this report should be able to fit a polynomial function with $14$ inputs, therefore, it does not make sense to penalise coefficients here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0919b-7aa5-41e2-9553-ff9ee18b896f",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "During training of each classifier a total of $14$ predictor variables were fed as inputs for training. While this is by no means a high number of input variables, it would be impractical to plot every pairwise combination of the features as a decision boundary as there would simply be too many plots to show and analyse. We note that decision boundary plots can only really be shown in 2 dimensions (at least for now or without AR!). Instead, using the technique described in the **Ranking of Predictor Variables** section, only the top $\\delta = 5$ variables were considered for their decision boundary. We are interested in the top ranking predictor variables because they are the ones most likely to show a manifold like structure of the data.\n",
    "\n",
    "When plotting pairs of variables, the remaining features are held constant at their respective mean. This does have limitations in that some plots may misrepresent the boundaries, particularly if the size of each class isn't equal, however, it allows us to visualise the model in an easy to understand way.\n",
    "\n",
    "I would argue holding the non-plotted variables at their mean is superior to ploting e.g. PCA because the former method does not cause loss of interpretability of the data. From the plots below we can clearly see the regions of the classes learned by the model with respect to specific variables. Using a dimensionality reduction technique does not make sense here given the already small number of features. In the case of hundreds or thousands of input variables where many of them turn to be important, then PCA could be considered useful, however, it was not used in this report for the reasons specified above.\n",
    "\n",
    "In the below section, the performance of the final classifiers for both output variables is discussed, along with the the corresponding decision boundary plots according to the methodology derived above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023d5aa-4928-4ac7-8135-879be159c0d5",
   "metadata": {},
   "source": [
    "## Species\n",
    "\n",
    "Firstly, we will consider only the decision boundaries of the `Species` output variable. The top predictor variables derived from the decision tree are ranked below\n",
    "\n",
    "\n",
    "1. `w1`\n",
    "2. `w2`\n",
    "3. `Thorax_length`\n",
    "4. `wing_loading`\n",
    "5. `w3`\n",
    "6. `Vial`\n",
    "7. `Replicate`\n",
    "8. `Sex`\n",
    "9. `Temperature`\n",
    "\n",
    "\n",
    "In the below discussion of the each model, the decision boundary plots show the top $4$ of these features plotted pairwise, with the plotted points being the test set where the colour represents their prediction and the background representing the model's prediction across the meshgrid of the plot. Note the colours do not represent true class of the test points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311be88-31cc-41e3-83f8-59cb3e4a0f6d",
   "metadata": {},
   "source": [
    "### $k$-NN\n",
    "\n",
    "The decision boundary plot of the final $k$-NN classifier on the `Species` variable with $k = 9$ is shown below.\n",
    "\n",
    "<img src=\"..\\plots\\knn_Thorax_decision_boundaries_Species.png\" width=\"700\">\n",
    "\n",
    "It is evident from the plot above that `Thorax Length` and `Wing Loading` help to differentiate the two species the best.\n",
    "\n",
    "The remaining plots show `Temperature`, `w1` and `w2` do not explain the differences between the two species alone. This is intruiging given that `w1` and `w2` were listed as the top-two predictors by the decision tree. We are, however, limited in that the decision tree ranks variables according to how it best classifies the data, and $k$-NN is a fundamentally different approach to classification here.\n",
    "\n",
    "It is evident that in all but the top-right plot, the boundaries are always parallel with one of the axes (of which is either `Thorax Length` or `Wing Loading`), although `Temperature` is a little better here. This inidicates the $k$-NN model is relying heavily on `Thorax Length` and `Wing Loading` to make predictions, and surprisingly achieves train accuracy of $72.2\\%$ and test accuracy of $64.0\\%$. The significant lack of clear class clusters does put this into question, though there is a gap of only $7.8\\%$ between train and test indicating that, though significantly overlapping, perhaps this is the true distribution of the `Species` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69cd2d-931e-4453-b445-0ea19f8f3a84",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "<img src=\"..\\plots\\dt_Thorax_decision_boundaries_Species.png\" width=\"700\">\n",
    "\n",
    "Interestingly the decision tree classifier performed very well achiving $92.5\\%$ train and $72.5\\%$ test accuracy when classifying `species`. This very high accuracy is somewhat reflected in the decision boundary plot above, though it doesn't appear obvious the model has captured a clear distribution of the data. This is further evident by the generalisation gap.\n",
    "\n",
    "There is, however, some structure, notably in the bottom-right plot where `D._buzzatti` (green) has a visually smaller variance with respect to `Wing Loading` compared with `D._aldrichi` (red). It can be seen the decision tree has picked the extremeties of the scale as the latter because of the distinct lack of `D._buzzatti` data points there.\n",
    "\n",
    "In the middle-left plot, it does appear there are slightly more `D._aldrichi` data points than `D._buzzatti`, and accordingly the model has placed a decision boundary in between these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584f18f-0e26-4a05-86b4-4e7fd51522b6",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "<img src=\"..\\plots\\nn_Thorax_decision_boundaries_Species.png\" width=\"700\">\n",
    "\n",
    "As expected for a parametric model, the neural netork learned a far smoother function than the non-parametric counterparts. This is indicated by the continuous decision boundary in the bottom-right plot (`Temperature` vs `wing loading`). Looking at the overlayed test points, this boundary appears to be the only one showing a clear difference between the two species. On the left we see more `D._buzzatti` and on the right we see more `D._aldrichi`.\n",
    "\n",
    "Contrast this with the plots involving `l2` and `w2` where it appears `D._aldrichi` is contained within `D._buzzatti` having the same or similar mean but a smaller variance. Unlike the decision tree model, the neural network is limited to a the constraints of a parametric function as a decision boundary. This does appear to hurt performance of the model with the NN achieving a train accuracy (averaged over the final $20,000$ epochs) of $44.4\\%$ and a test accuracy of $42.6\\%$. A plot showing the train and validation accuracies during training is show below.\n",
    "\n",
    "<img src=\"..\\plots\\nn_train_val_accuracies.png\" width=\"700\">\n",
    "\n",
    "While this accuracy is quite low, it is evident from the decision boundary plots above that the two species do not show a clear difference in any of the features other than perhaps `wing loading` and `temperature`. This explains the poor accuracy, though it is good to see here that the generalisation gap between train and test is quite small, indicating the model has not overfitted to the train data.\n",
    "\n",
    "As discussed in the data preprocessing section, if `Longitude` and `Lattitude` were included in the features, $X$, the model achieved over $90\\%$ train and $85\\%$ test accuracy, which corroborates the finding that these features were fixed for each class within the `Population`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6690599-298a-451e-b735-4747b8a0c53f",
   "metadata": {},
   "source": [
    "## Population\n",
    "\n",
    "Secondly, we will consider only the decision boundaries of the `Population` output variable for each classifier. The top predictor variables derived from the decision tree are ranked below\n",
    "\n",
    "1. `w1`\n",
    "2. `wing_loading`\n",
    "3. `w2`\n",
    "4. `Thorax_length`\n",
    "5. `w3`\n",
    "6. `Vial`\n",
    "7. `Sex`\n",
    "8. `Replicate`\n",
    "9. `Temperature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b8bc0-1bf9-4b34-ac10-99b3cdf56614",
   "metadata": {},
   "source": [
    "### $k$-NN\n",
    "\n",
    "The decision boundary plot of the final $k$-NN classifier on the `Population` variable with $k = 18$ is shown below.\n",
    "\n",
    "<img src=\"..\\plots\\knn_Thorax_decision_boundaries_Population.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcd715-e046-4e88-89ea-e6c3e2e42340",
   "metadata": {},
   "source": [
    "From the array of decision boundary plots above, it is evident the final $k$-NN model for predicting `Population` saw a similar output compared with the `Species` final $k$-NN mode. We see again that `Thorax Length` and `wing_loading` were the two best predictors as evident by the fact the decision regions have boundaries only with respect to these features. However, there are many more populations (5) than species (2), and hence the plot appears far more complex.\n",
    "\n",
    "The final model achieved a train accuracy of $31.0\\%$ and a test accuracy of $22.7\\%$. This is much lower than the `Species` $k$-NN classifier despite a similar looking decision boundary. Although the plot of `Thorax length` and `wing_loading` may appear interesting, given the low accuracy and lack of unique clusters for any given class, the model did not actually perform well at distinguishing between different populations. We do see different regions of classes within the plot, however, these could perhaps be due to clusters of a class that simply happen to be present in the training set. \n",
    "\n",
    "Having said that, it is evident there is more `Oxford Downs` in the low region of wing loading compared with the high region. Similarly `Goganogo Creek` and `Binjour` trend to having a lower and higher `Thorax Length` respectively, albeit very loose trends.\n",
    "\n",
    "Finally, it is also evident the `w1`, `w2`, and `w3` variables do not provide separation of the `Population` classes. This is supported by the decision boundaries parallel with these axes in the remaining plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2c132-6888-4785-b31a-8478d7a7fc7e",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "<img src=\"..\\plots\\dt_Thorax_decision_boundaries_Population.png\" width=\"700\">\n",
    "\n",
    "When trained for classifying `Population` the decision tree model did not perform nearly as well as it did with `Species`. In training it achieved $63.7\\%$ accuracy and in testing, just $22.3\\%$. Again, this may come down to tweaking hyperparameters, though it is clear from the above decision boundary plots that the data lacked structure for the model to train to.\n",
    "\n",
    "Unlike $k$-NN which has the ability to identify small clusters, the decision tree classifer does not have this ability. The `wing loading` vs `Thorax Length` plot is  the best view which shows the model is essentially guessing the class - all 5 classes are overlapping with little to no sense of structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02714d85-cc96-4eca-ba20-04103563ce25",
   "metadata": {},
   "source": [
    "Similarly to the results of the decision tree in the `Species` classification, the model did not perform very well for `Population` either. We can see it does make decision boundaries with borders in the center of the data, however, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a260a1-7364-4c48-acce-15b33e604034",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "<img src=\"..\\plots\\nn_Thorax_decision_boundaries_Population.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b94329-393d-4ecf-bda1-c059f58fc96e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The neural network model also performed poorly on the Population. While the accuracy is combined with species and so we cannot tell with this data alone which output variable is most responsible, it is evient from the plots above, the high level of overlap did not allow a clear distinction between the classes. Only `wing loading` was particularly useful at separating classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6fa83-84f1-4d76-a6b2-ef1cd89fbc46",
   "metadata": {},
   "source": [
    "# Ranking of Models\n",
    "\n",
    "From the results below, it might appear evident that the Decision tree model performed the best, at least for classifying species. While the accuracy may tell us this, as discussed above in the results section, it does not appear the model captured the distribtion of the data. On the other hand, the $k$-NN model did appear to capture structure in the data, as most evidently highlited by the `Wing loading` vs `Thorax length` visualisation. Despite this, $k$-NN performed worse when classifying species compared with the decision tree, though much better than the neural network. Primarily this resulted from $k$-NN's ability to identify small clusters in the data, since it is a non-parametric model.\n",
    "\n",
    "When predicting `Population` however, the neural network outperformed both non-parametric models by almost double the test accuracy. It appears there is actually some structure in the population classes, and while in the decision boundary plot it might appear $k$-NN has captured the clusters, in reality these are nothing more than \"hotspots\" in the data where there just happens to be more of a particular class than others. Even a high value of $k = 18$ did not circumvent this fundamental limitation of the model. Similarly, the decision tree identified smaller regions (though larger than $k$-NNs) of different populations, but these were heavly overlapped leading to a high train accuracy but a much lower test accuracy.\n",
    "\n",
    "\n",
    "| Model | Species Train Accuracy | Species Test Accuracy | Population Train Accuracy | Population Test Accuracy |\n",
    "|-------|------------------------|-----------------------|---------------------------|--------------------------|\n",
    "| $k$-NN            |  $70.0\\%$  |       $61.3\\%$        |           $30.6\\%$        |         $18.9\\%$         |\n",
    "| Decision Tree     |  $83.3\\%$  |       $77.1\\%$        |           $62.6\\%$        |         $27.7\\%$         |\n",
    "| Neural Network    |  $44.4\\%$  |       $42.6\\%$        |           $44.4\\%$        |         $42.6\\%$         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbb187-dddd-47bd-b29b-9efea51f2397",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf2221-072d-438f-bee1-39175df13f11",
   "metadata": {},
   "source": [
    "In conclusion this report into classifying `Species` and `Population` from the Thorax & wing traits dataset revealed that classifying these features is very difficult given the data available. It highlighted the strengths and weaknesses of three fundamentally different models. Firstly $k$-NN did produce the best results even in testing which further indicates the lack of distinct clusters between the classes of either output variable. The $k$-NN model was able to achieve good results under these conditions becasue of its ability to identify smaller clusters. In contrast the decision tree model wasn't able to do this, though it was able to construct a decision boundary between the two `Species` where the variance in the data was different but the means were the same. Finally, the neural network\n",
    "\n",
    "## Improvements\n",
    "\n",
    "The decision tree model did achieve a high train accuracy, however, it's test accuracy in classifying both `Species` and `Population` (notably the latter) was much lower. This indicates the model was potentially overfitted to the data. In future, performing cross validation to obtain an optimal max tree depth via a grid search may be a good step.\n",
    "\n",
    "Additionally, the decision tree's high performance in classifying species would benefit from further investigation, perhaps using bagging (e.g. a Random Forest) to corroborate this finding, as it does not appear clear from the visualisations presented that the model captured the underlying distribution of the species data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8077fe-5dad-4768-94bc-e070d78f1d83",
   "metadata": {},
   "source": [
    "# References (yet to compile)\n",
    "\n",
    "1. https://www.agriculture.gov.au/biosecurity-trade/pests-diseases-weeds/fruit-flies-australia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
